{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Sentinel-1  Time Series\n",
    "\n",
    "SAR data are not especially good for vegetation classification. However they have the great advantage of being independent of cloud cover. Here we investigate the use of S1 time series over a complete growth period for thematic mapping.\n",
    "\n",
    "Since we have no ground truth data, we will use the Canada AAFC Annual Crop Inventory, which is also on the GEE archive. In particular the 2017 inventory for an area in southern Saskatchewan. This area consists of large agricultural fields, well-defined crops, and flat terrain (a big advantage for SAR measurement).\n",
    "\n",
    "Multilook SAR image data are not normally distributed, rather they are gamma distributed. The GEE classifiers might be expected not to work so well, so we will use Tensorflow to program a more flexible neural network classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First of all we grab a time series for the region of interest over the 2017 growing season (March to October):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import ee\n",
    "from auxil.eeWishart import omnibus\n",
    "ee.Initialize()\n",
    "poly = ee.Geometry({'geodesic': False, 'type': 'Polygon', 'coordinates': [[[-105.10328600000001, 50.24327999999998], \n",
    "                                                                           [-104.66649400000001, 50.24327999999998], \n",
    "                                                                           [-104.66649400000001, 50.46604134048255], \n",
    "                                                                           [-105.10328600000001, 50.46604134048255], \n",
    "                                                                           [-105.10328600000001, 50.24327999999998]]]})\n",
    "collection = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT') \\\n",
    "                      .filterBounds(poly) \\\n",
    "                      .filterDate(ee.Date('2017-03-01'), ee.Date('2017-11-01')) \\\n",
    "                      .filter(ee.Filter.eq('transmitterReceiverPolarisation', ['VV','VH'])) \\\n",
    "                      .filter(ee.Filter.eq('resolution_meters', 10)) \\\n",
    "                      .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "                      .filter(ee.Filter.eq('relativeOrbitNumber_start',5)) \\\n",
    "                      .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))  \n",
    "collection = collection.sort('system:time_start')\n",
    "\n",
    "collection.size().getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Next, we transform the collection into a multiband image with linear intensities, pre-processed with the refined Lee filter:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auxil.eeRL import refinedLee\n",
    "import math\n",
    "\n",
    "def get_vvvh(image):   \n",
    "    ''' get 'VV' and 'VH' bands from sentinel-1 imageCollection '''\n",
    "    return image.select('VV','VH')\n",
    "\n",
    "timeseries = collection \\\n",
    "            .map(get_vvvh) \\\n",
    "            .map(refinedLee) \\\n",
    "            .toBands() \\\n",
    "            .clip(poly) \\\n",
    "            .float() \n",
    "\n",
    "timeseries.bandNames().length().getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The class lables are conveniently obtained from the GEE archive of the Canadian AAFC Annual Crop Inventory for the year 2017, and we append them as an additional band (band 39):__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop2017 = ee.ImageCollection('AAFC/ACI') \\\n",
    "    .filter(ee.Filter.date('2017-01-01', '2017-12-01')) \\\n",
    "    .first() \\\n",
    "    .clip(poly)\\\n",
    "    .float()\n",
    "labeled_timeseries = ee.Image.cat(timeseries,crop2017)\n",
    "\n",
    "labeled_timeseries.bandNames().length().getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now export the image to the Google drive (cloud storage would be better, but I don't have a billing account). Note that the export scale is 30m corrresponding to the AAFC/ACI resolution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drexport = ee.batch.Export.image.toDrive(labeled_timeseries,\n",
    "                  description='driveExportTask', \n",
    "                  folder = 'EarthEngineImages',\n",
    "                  fileNamePrefix='labeled_timeseries',scale=30,maxPixels=1e10)\n",
    "drexport.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__After downloading from the drive to a local directory, we have:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l imagery/regina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This displays three of the (filtered) VV bands and the last (label) band:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/dispms -f imagery/regina/labeled_timeseries.tif  -p [21,23,25] \\\n",
    "-F imagery/regina/labeled_timeseries_rl.tif -E 2 -P [57,57,57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__this displays three of the bmap bands__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/dispms -f imagery/regina/labeled_timeseries.tif -p [52,53,54] -e 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now read the labeled time series into a Numpy array, which we will use to train a Tensorflow NN classifier:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from osgeo.gdalconst import GA_ReadOnly, GDT_Byte\n",
    "import numpy as np\n",
    "\n",
    "gdal.AllRegister()                   \n",
    "inDataset = gdal.Open('imagery/regina/labeled_timeseries.tif',GA_ReadOnly)\n",
    "cols = inDataset.RasterXSize\n",
    "rows = inDataset.RasterYSize    \n",
    "bands = inDataset.RasterCount\n",
    "labeled_timeseries = np.zeros((rows*cols,bands))                              \n",
    "for b in range(bands):\n",
    "    band = inDataset.GetRasterBand(b+1)\n",
    "    labeled_timeseries[:,b]=band.ReadAsArray(0,0,cols,rows).ravel()   \n",
    "labeled_timeseries = np.nan_to_num(labeled_timeseries)   \n",
    "\n",
    "#multiply the intensities by 100\n",
    "labeled_timeseries[:,:38] *= 100\n",
    "\n",
    "# for later\n",
    "driver = inDataset.GetDriver() \n",
    "m = labeled_timeseries.shape[0] \n",
    "inDataset = None\n",
    "\n",
    "labeled_timeseries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The AAFC/ACI thematic maps have 68 different classes. This code generates a dictionary of class names:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classdict = {'0':'Nc'}\n",
    "filepath = 'imagery/AAFC.txt'\n",
    "with open(filepath) as fp:\n",
    "    line = fp.readline()\n",
    "    key = line[:3].replace('\\t','')\n",
    "    value = line[10:].replace('\\t',' ').replace('\\n','')\n",
    "    classdict.update({key:value})\n",
    "    while line:\n",
    "        line = fp.readline()\n",
    "        key = line[:3].replace('\\t','')\n",
    "        value = line[10:].replace('\\t','').replace('\\n','')\n",
    "        classdict.update({key:value})\n",
    "del classdict['']\n",
    "\n",
    "len(classdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we can see which class labels pertain to our region of interest:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnums = np.unique(labeled_timeseries[:,-1])\n",
    "print(classnums)\n",
    "classnames = str([classdict[str(int(cn))] for cn in classnums])\n",
    "classnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In order to train the neural network we have to renumber the labels consecutively from 0:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "labels = labeled_timeseries[:,-1]\n",
    "for c in classnums:\n",
    "    labels = np.where(labels==c,i,labels)\n",
    "    i += 1  \n",
    "labels = np.array(labels,dtype=np.uint8) \n",
    "n_classes = len(np.unique(labels))\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write the labels as an image to disk and display them:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDataset = driver.Create('imagery/regina/labels.tif',cols,rows,1,GDT_Byte)\n",
    "outBand = outDataset.GetRasterBand(1)\n",
    "outBand.WriteArray(np.reshape(labels,(rows,cols)))\n",
    "outBand.FlushCache()\n",
    "outDataset = None\n",
    "%run scripts/dispms -f imagery/regina/labels.tif -c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we simulate ground truth by taking a random subset of training pixels:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random subset for training\n",
    "np.random.seed(123)\n",
    "n = 50000\n",
    "idx = np.random.permutation(m)[0:n]\n",
    "\n",
    "# just the intensities (x 100)\n",
    "Xs1 = labeled_timeseries[idx,:38]\n",
    "\n",
    "# just the significant changes \n",
    "Xs2 = labeled_timeseries[idx,39:-1]\n",
    "\n",
    "# all training vectors\n",
    "Xs = labeled_timeseries[idx,:-1]\n",
    "\n",
    "# one hot encoded class labels\n",
    "Ls = np.array(labels[idx],dtype=np.int)\n",
    "ls = np.zeros((n,n_classes))\n",
    "for i in range(n):\n",
    "    ls[i,Ls[i]] = 1\n",
    "print(ls[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The module class auxil.dnn encapsulates a simple feed forward neural network using tf.keras.models.Sequential(). We use it with two hidden layers, each with 40 neurons, to classifiy the series of intensities (Xs1):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import auxil.dnn as dnn\n",
    "\n",
    "classifier = dnn.Dnn([40,40],n_classes,learning_rate=0.002)\n",
    "classifier.train(Xs1,ls,epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The accuracy on the training and validation data is about 86%:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we include the times of significant change in the input vectors (Xs):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import auxil.dnn as dnn\n",
    "\n",
    "classifier = dnn.Dnn([40,40],n_classes,learning_rate=0.002)\n",
    "classifier.train(Xs,ls,epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We'll use the result to classify (predict) the entire image:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls,probs = classifier.classify(labeled_timeseries[:,0:-1]) \n",
    "# for later display:\n",
    "cls[0]=1\n",
    "cls[1]=n_classes-1\n",
    "\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write the thematic map and the class probabilities images to disk:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the class image to disk\n",
    "outDataset = driver.Create('imagery/regina/timeseries_class.tif',cols,rows,1,GDT_Byte)\n",
    "outBand = outDataset.GetRasterBand(1)\n",
    "outBand.WriteArray(np.reshape(cls,(rows,cols)))\n",
    "outBand.FlushCache()\n",
    "outDataset = None\n",
    "# write the class probabilities to disk\n",
    "bands = probs.shape[1]\n",
    "probs = np.byte(probs*255)\n",
    "outDataset = driver.Create('imagery/regina/timeseries_probs.tif',cols,rows,bands,GDT_Byte)\n",
    "for b in range(bands):\n",
    "    outBand = outDataset.GetRasterBand(b+1)\n",
    "    outBand.WriteArray(np.reshape(probs[:,b],(rows,cols)))\n",
    "    outBand.FlushCache()\n",
    "outDataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test the classifier with all of the training data (misclassification rate about 14.5%):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.test(Xs,ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compare the classified image with the AAFC/ACI thematic map:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/dispms -f imagery/regina/timeseries_class.tif -c -F imagery/regina/labels.tif -C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Looks good, but we can \"pretty it up\" some more with Probabilistic Label Relaxation (see Chapter 7 in my <a href= \"https://www.amazon.com/Analysis-Classification-Change-Detection-Sensing/dp/1138613223/ref=dp_ob_title_bk\">textbook</a>):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/plr imagery/regina/timeseries_probs.tif\n",
    "\n",
    "%run scripts/dispms -f imagery/regina/timeseries_probs_plr.tif -c  \\\n",
    "                    -F imagery/regina/labels.tif -C -s '/home/mort/Bilder/tmp.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
